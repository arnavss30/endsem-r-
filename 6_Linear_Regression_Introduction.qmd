---
title: "Introduction to Linear Regression"
author: "Dr. Balu Pawde"
format:
  html:
    toc: true
date: today
editor: visual
---

------------------------------------------------------------------------

> Note: This writeup introduces to the concept of linear regression[^1].

[^1]: Book: 'Introduction to Econometrics with R' written by Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer, 2024

------------------------------------------------------------------------

# Introduction

-   The subject matter of `Econometrics` involves developing statistical methods for estimation of statistics
    -   What is the impact of campaigning expenditure on voting/
    -   What is the forecast of GDP
    -   What is the effect of school spending on performance in terms of career, etc.
-   Econometrics evolved from mathematical statistics

------------------------------------------------------------------------

# Structure of Economic Data

-   [**Structures: Cross section, Time series, Pooled cross-section, Panel data**]{.underline}
-   **Cross section**
    -   Sample of units at a point in time
    -   Often assume random sampling
    -   Most useful in microeconomics
    -   Example:

![Example of cross-section data](Cross_section_example.png){fig-align="center"}

-   **Time series**
    -   Data on variable/s over time
    -   Example: stock prices, money supply, CPI
    -   IMP in time series: Past events can influence future events - lag in behaviour
    -   Unlike cross section, observations are rarely independent over time in time series
        -   To remedy this dependence we need to think about type of dependence, trends and seasonality
    -   Frequency in time series is important to pay attention to.
    -   Example:

![Example of time-series data](Time_series_example.png){fig-align="center"}

-   **Pooled cross-section**
    -   Cross-sectional and time series features
    -   Two separate random samples at different times
    -   Usually used to increase the sample size
    -   Useful to see how a relationship has changed over time
    -   Example:

![Example of pooled cross-section data](Pooled_cross_section_example.png){fig-align="center"}

-   **Panel data**
    -   Time series for each cross section observation or member
    -   Unlike pooled cross-section, panel data has same units (observations) followed over time
    -   Panel data allows additional learning
    -   Panel data needs transformation and therefore has a particular ordering of the observations
    -   Example:

![Example of panel data](Panel_data_example.png){fig-align="center"}

------------------------------------------------------------------------

# Causality, Ceteris Paribus and Counterfactual

-   Causality
    -   Economist's goal: one variable has a `causal effect` on the other variable
    -   Simple association (such as correlation; visualization) can be suggestive but rarely compelling. Therefore causality is needed.
-   Ceteris Paribus: other factors being equal/constant
    -   This assumption is required for causality
    -   It is not possible to hold everything else constant
    -   The counterfactual thinking helps thinking about ceteris paribus

------------------------------------------------------------------------

# Simple Regression Model

-   In regression analysis, we are interested in explaining $y$ in terms of $x$, where $y$ and $x$ come from the same population
-   $3$ issues in such relationship:
    -   How do we allow other factors to affect $y$
    -   What is the functional relationship between $y$ and $x$
    -   How to be sure about capturing ceteris paribus relationship
-   These $3$ issues can be addressed by writing down $$y=\beta_0+\beta_1x+u$$ which is simple linear regression model (SLRM) or bivariate linear model. Here,
    -   the $y$ is explained variable
    -   the $x$ is explanatory variable,
    -   the $u$ is error/disturbance term: answers
        -   [How to allow effect of other factors]{.underline}.
        -   [Also, allows us to think about ceteris paribus]{.underline}
    -   the functional form is linear
    -   the $\frac{\delta y}{\delta x}=\beta_1$ and $\Delta y=\beta_1 \Delta x$
    -   The linearity in this model implies that the effect of change in $x$ on $y$ is same regardless of initial value of $x$.
-   Regression equation:
    -   $u$ and $x$ are random variables
    -   Once $\beta_0$, i.e. the constant term is included in the equation, nothing is lost if we assume $E(u)=0$
    -   We can assume that average value of $u$ does not depend on $x$: $E(u|x)=E(u)$, which implies $$E(u|x)=0$$
    -   This renders a useful interpretation of $\beta_0$: $$E(y|x)=\beta_0+\beta_1 x$$ which shows how average $y$ changes with $x$
-   The regression equation $y=\beta_0+\beta_1x+u$ can now be broken into $2$ parts:
    -   Systematic part: $y=\beta_0+\beta_1x$ and
    -   Unsystematic part: $u$

------------------------------------------------------------------------

# OLS estimates

-   The most important for us in the equation $y=\beta_0+\beta_1x+u$ are the coefficients: $\beta_0$ and $\beta_1$
    -   These cannot be estimated from the population

    -   So, we need a sample: $\{ (x_i , y_i):i=1,...,n \}$ which comes from the population regression function (PRF): $$y=\beta_0+\beta_1x+u$$

    -   Each observation comes the PRF and hence it can be written as: $$y_i=\beta_0+\beta_1x_i+u_i$$

    -   We have assumed that in the population $u$ is uncorrelated with $x$; it implies

        $E(u)=0$ \[eqn. 1\]

        $cov(x,u)=E(x,y)=0$ \[eqn. 2\]

    -   \[eqn. 1\] can be rewritten as $$E(y-\beta_0-\beta_1x)=0$$ (call it \[eqn. 3\]) since $u=y-\beta_0-\beta_1x$

    -   \[eqn. 2\] can be rewritten as $$E[x(y-\beta_0-\beta_1x)]=0$$ (call it \[eqn. 4\])

    -   The \[eqn. 3 and 4\] are the population equations. What are the sample counterparts?

        -   For eqn. 3: $$n^{-1} \Sigma^{n}_{i=1}(y_i - \hat{\beta_0}-\hat{\beta_1}x_i)=0$$ (call the above equation \[eqn. 5\])

        -   For eqn. 4: $$n^{-1} \Sigma^{n}_{i=1} x_i (y_i - \hat{\beta_0}-\hat{\beta_1}x_i)=0$$ (call the above equation \[eqn. 6\])

        -   Now we have two equations in two unknowns. We can solve for $\hat{\beta_0}$ and $\hat{\beta_1}$

        -   \[Eqn. 5\] can be written as $$\overline{y}= \hat{\beta_0}+\hat{\beta_1}\overline{x}$$ or $$\hat{\beta_0}= \overline{y}-\hat{\beta_1}\overline{x}$$ (call the above equation \[eqn. 7\]). This suggests that if we have estiamte of $\hat{\beta_1}$, getting estimate of \hat{\beta_0} is straightforward.

        -   substituting this value of $\beta_0$ from \[eqn. 7\] in \[eqn. 6\], we get $$ \Sigma^{n}_{i=1} x_i (y_i - (\overline{y}-\hat{\beta_1}\overline{x})-\hat{\beta_1}x_i)=0$$ which can be rearranged as $$\Sigma^{n}_{i=1} x_i (y_i - \overline{y} - \hat{\beta_1}x_i - \hat{\beta_1}\overline{x})=0$$ which becomes $$\Sigma^{n}_{i=1} x_i (y_i - \overline{y}) = \hat{\beta_1} \Sigma^{n}_{i=1} x_i(x_i - \overline{x})$$ which upon rearrangement becomes $$\hat{\beta_1}= \frac{\Sigma^{n}_{i=1} x_i (y_i - \overline{y})} {\Sigma^{n}_{i=1} x_i(x_i - \overline{x})}$$

        Since $[\Sigma^{n}_{i=1} x_i(y_i - \overline{y})=\Sigma^{n}_{i=1} (x_i - \overline{x})(y_i - \overline{y})]$ and $\Sigma^{n}_{i=1} x_i(x_i - \overline{x}) = (x_i - \overline{x})^2]$

$$\hat{\beta_1}= \frac{\Sigma^{n}_{i=1} (x_i - \overline{x})(y_i - \overline{y})} {\Sigma^{n}_{i=1} (x_i - \overline{x})^2} = \frac{cov(x,y)}{var(x)} = \hat{\rho}*\frac{\hat{\sigma_y}}{\hat{\sigma_x}}$$ (call the above equation \[eqn. 8\])

-   It follows that if the correlation b/w $y$ and $x$ is $+ve$ then $\hat{\beta_1}>0$ and vice-versa

-   The only assumption in achieving the estimates of $\hat{\beta_0}$ and $\hat{\beta_1}$ is that ${\Sigma^{n}_{i=1} (x_i - \overline{x})^2}>0$ which fails only if there is no variation in $x$

-   The estimates in \[eqn. 7 and 8\] are called `ordinary least squares` estimates.

-   With the estimates of $\hat{\beta_0}$ and $\hat{\beta_1}$ we can estimate form the OLS regression line $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$

-   The slope estiamte can be written as $\hat{\beta_1} = \frac{\Delta \hat{y}}{\Delta \hat{x}}$ and $\Delta \hat{y} = \hat{\beta_1}{\Delta \hat{x}}$

------------------------------------------------------------------------

# Properties of OLS fitted line

-   By construction $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$ and $y = \hat{y} + \hat{u}$ i.e, $$\hat{u} = y - \hat{y}$$ So, if $\hat{u}>0$, the model underpredicts.
-   The sum of residuals $\Sigma_{i=1}^n \hat{u_i} = 0$; implying average of $\hat{u_i} = 0$
-   The sample correlation b/w regressor and residuals is zero, i.e. $$\Sigma_{i=1}^n {x_i}{\hat{u_i}} = 0$$
-   The point $(\bar{x},\bar{y})$ is always on the OLS regression line
-   Given that $y_i = \hat{y_i} + \hat{u_i}$ and $E(\hat{u_i})=0$, it implies that $\overline{y_i} = \overline{\hat{y_i}}$
    -   This implies that the sample covariance between $\hat{y_i}$ and $\hat{u_i}$ is zero.

------------------------------------------------------------------------

# SST, SSE and SSR

-   The $SST = \Sigma^{n}_{i=1}  (y_i - \overline{y})^2$
    -   SST measures sample variation in $y_i$
-   The $SSE = \Sigma^{n}_{i=1} (\hat{y_i} - \overline{y})^2$
    -   SST measures sample variation in $\hat{y_i}$
-   The $SSR = \Sigma^{n}_{i=1} (\hat{y_i})^2$
    -   SST measures sample variation in $(\hat{u_i})^2$

$$SST = SSE + SSR$$

(Call the above eqn. \[9\]\])

------------------------------------------------------------------------

# Goodness of fit

-   Refers to
    -   How well does the explanatory variable explain the explained variable?
    -   How well does the OLS line fit the data?
    -   If we divide eqn. \[9\] by $SST$, we get: $1 = \frac{SSE}{SST} + \frac{SSR}{SST}$
    -   The measure `R-squared` is defined as $R^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}$
    -   `R-squared` tells us the fraction of variation in $y$ that is explained by $x$
    -   The $R^2 = corr(y_i, \hat{y_i})$

------------------------------------------------------------------------

# Units of measurement and functional form

-   scaling of the dependent variable (i.4. $y$) (multiplied by $c$) results in intercept and slope coefficient being multiplied by the same $c$
-   If IndepVar $x$ is multiplied by $c$ then slope coefficient is multiplied by $c$ without any change in intercept.
-   `R-squared` does not depend on the units of measurement.

------------------------------------------------------------------------

# Incorporating non-linearities in SLRM

-   Two common cases of non-linearity
    -   logarithm of Depvar: $log(w) = \beta_0 + \beta_1 edu + u$
        -   The interpretation in this case is in terms of $\% \Delta$
            -   $\% \Delta wage = (100 \times \beta_1) \Delta edu$
        -   Logarithm is often used to impose constant $\% change$ interpretation
    -   logarithm of both vars: $log(salary) = \beta_0 + \beta_1 log(sales) + u$
        -   The interpretation in this case is in terms of elasticity of salary w.r.t. sales
            -   $1\% \Delta$ in sales induces $\beta_1\% \Delta$ in salary

------------------------------------------------------------------------

# Estimating model with R

```{r, warning=FALSE, message=FALSE}
 install.packages("AER")
 install.packages("MASS")
```

```{r, warning=FALSE, message=FALSE}
library(AER)
library(MASS)

# load the the data set in the workspace
data(CASchools)
```

```{r}
# Check the class of the dataset
class(CASchools)
# data frame is a convenient type of data to work in the regression analysis
```

## Examining data

```{r}
# Get the feel of the data
#head(CASchools)
#View(CASchools)
str(CASchools)
# str gives more comprehensive overview of the data
View(CASchools)
```

-   We are interested in estimating the effect of student-teacher ratio (`STR`) on average test score (`score`).
    -   These variables are not available in the data but can be computed.
    -   To obtain the student-teacher ratios, we simply divide the number of students by the number of teachers.
    -   The average test score is the arithmetic mean of the test score for reading and the score of the math test.

```{r}
# compute STR and append it to CASchools
CASchools$STR <- CASchools$students/CASchools$teachers 

# compute score and append it to CASchools
CASchools$score <- (CASchools$read + CASchools$math)/2     
```

```{r}
# let's check if the above variables are added to the dataframe or not
head(CASchools)
```

## Summary Statistics

-   It is always a good idea to examine your data using summary statistics

```{r}
# compute sample averages of STR and score
avg_STR <- mean(CASchools$STR) 
avg_score <- mean(CASchools$score)
```

```{r}
# compute sample standard deviations of STR and score
sd_STR <- sd(CASchools$STR) 
sd_score <- sd(CASchools$score)
```

```{r}
# set up a vector of percentiles and compute the quantiles 
quantiles <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_STR <- quantile(CASchools$STR, quantiles)
quant_score <- quantile(CASchools$score, quantiles)
```

```{r}
# gather everything in a data.frame 
DistributionSummary <- data.frame(Average = c(avg_STR, avg_score), 
                                  StandardDeviation = c(sd_STR, sd_score), 
                                  quantile = rbind(quant_STR, quant_score))
```

```{r}
# print the summary to the console
DistributionSummary
```

------------------------------------------------------------------------

## Visualization

-   Similarly, it is useful to visualize the data at hand

```{r}
library(ggplot2)

ggplot(data = CASchools,
  mapping = aes(x = STR, y = score)) +
  geom_point(shape = 1, size = 3 )+
  labs(
    x = "STR (X)",
    y = "Test Score (Y)",
    title = "Scatterplot of Test Score and STR")+
  theme_bw()

# Observation from the graph: The points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower test scores in bigger classes.
```

## Correlation

-   The function `cor()` can be used to compute the correlation between two numeric vectors.

```{r}
cor(CASchools$STR, CASchools$score)
# Is the correlation negative as shown by the association in graph?
# is the correlation strong or weak?
```

## Estimating regression

-   Currently, our task is to find the best line that fits the data.
    -   There can be multiple ways of fitting such lines, e.g. graphing multiple lines as earlier and finding the best one.
    -   Or relying on the correlation that we just computed.
    -   But OLS is more compelling as it fits the line as close to the observed data points.
-   There are many ways of computing the estimates of betas

```{r}
# We can compute directly using the formulas that we know already

attach(CASchools) 
# The above allows to use the variables contained in CASchools directly

# compute beta_1_hat
beta_1 <- sum((STR - mean(STR)) * (score - mean(score))) / sum((STR - mean(STR))^2)

# compute beta_0_hat
beta_0 <- mean(score) - beta_1 * mean(STR)

# print the results to the console
beta_1

beta_0
```

-   The most common way to estimate the model is to use inbuilt `lm` command
    -   The regression formula with the basic syntax `lm(y ~ x, data)` where `y` is the dependent variable and `x` the explanatory variable

```{r}
# estimate the model and assign the result to linear_model
linear_model <- lm(score ~ STR, data = CASchools)

# print the standard output of the estimated lm object to the console 
linear_model

# Observe the output closely and compare this output with the earlier two formulas: covariance based and correlation based.
```

-   Let us add the estimated regression line to the plot. This time we also enlarge the ranges of both axes by setting the arguments xlim and ylim.

```{r}
# library(ggplot2)

ggplot(data = CASchools,
  mapping = aes(x = STR, y = score)) +
  geom_point(shape = 1, size = 3) +
  xlim(min(STR), max(STR)) +
  ylim(min(score), max(score)) +
  geom_smooth(method=lm, se=FALSE) +
  labs(
    x = "STR (X)",
    y = "Test Score (Y)",
    title = "Scatterplot of Test Score and STR")+
  theme_bw()
```

## Measures of fit

```{r}
# We had estiamted the linear model above and saved the results as an object: linear_model
# We now use function summary to get the model summary
mod_summary <- summary(linear_model)
mod_summary

# Observe the value of multiple R-squared. How much of the variation is explained? 
# Residual standard error tells us: on average what is the deviation of the actual achieved test score and the regression line (estimated)
```

-   We can check whether `summary()` uses the same definitions for $R^2$ and RSE as we do when computing them manually.

```{r}
# compute R^2 manually

#Below is the sum of squares residual
SSR <- sum(mod_summary$residuals^2)
# Note above that summary function stores residuals as well

#Below is the the sum of squares total
SST <- sum((score - mean(score))^2)

# Following the formula for R-squared
R2 <- 1 - SSR/SST

# print the value to the console
R2
```

```{r}
# compute RSE manually
# Standard deviation of the residuals
n <- nrow(CASchools)
RSE <- sqrt(SSR / (n-2))

# print the value
RSE

# The unit of the residual is the same as the dependent variable
```

## Assumptions of OLS

-   The model that we are looking at is: $$Y_i = \beta_0 + \beta_1 x_i + u_i$$
-   Assumption 1: The Error Term has Conditional Mean of Zero
    -   No matter which value we choose for $x$, the error term $u$ must not show any systematic pattern
    -   We can simulate to understand this assumption

```{r}
# set a seed to make the results reproducible
set.seed(5)

# simulate the data 
X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 1)  

# the true relation  
Y <- X^2 + 2 * X + u                

# estimate a simple regression model 
mod_simple <- lm(Y ~ X)

# estimate a quadratic regression model
mod_quadratic <- lm( Y ~ X + I(X^2)) 

# predict using a quadratic model 
prediction <- predict(mod_quadratic, data.frame(X = sort(X)))

# plot the results
plot( Y ~ X, col = "black", pch = 20, xlab = "X", ylab = "Y")
abline( mod_simple, col = "blue",lwd=2)

#red line = incorrect linear regression (this violates the first OLS assumption)
lines( sort(X), prediction,col="red",lwd=2)
legend("topleft", 
       legend = c("Simple Regression Model", 
                  "Quadratic Model"),
       cex = 1,
       lty = 1,
       col = c("blue","red"))
```

-   Assumption 2: Independently and Identically Distributed Data
    -   same population, drawn independently

```{r}
# Violation of iid assumption below

# set seed
set.seed(5)

# generate a date vector
Date <- seq(as.Date("1951/1/1"), as.Date("2000/1/1"), "years")

# initialize the employment vector
X <- c(5000, rep(NA, length(Date)-1))

# generate time series observations with random influences
for (t in 2:length(Date)) {
  
    X[t] <- -50 + 0.98 * X[t-1] + rnorm(n = 1, sd = 200)
    
}

#plot the results
plot(x = Date, 
     y = X, 
     type = "l", 
     col = "steelblue", 
     ylab = "Workers", 
     xlab = "Time",
     lwd=2)

# Since the employment is dependent on the earlier point, the assumption of iid is violated.
```

-   Assumption 3: Large outliers are unlikely (finite Kurtosis)
    -   The outliers affect the fit, and therefore estimates of the betas and R-squared.
    -   Apparently typos, conversion errors or measurement errors.
    -   Even though genuine value, better to remove outliers to remove sensitivity

```{r}
# set seed
set.seed(123)

# generate the data
X <- sort(runif(10, min = 30, max = 70))
Y <- rnorm(10 , mean = 200, sd = 50)
Y[9] <- 2000

# fit model with outlier
fit <- lm(Y ~ X)

# fit model without outlier
fitWithoutOutlier <- lm(Y[-9] ~ X[-9])

# plot the results
plot(Y ~ X,pch=20)
abline(fit,lwd=2,col="blue")
abline(fitWithoutOutlier, col = "red",lwd=2)
legend("topleft", 
       legend = c("Model with Outlier", 
                  "Model without Outlier"),
       cex = 1,
       lty = 1,
       col = c("blue","red"))
```

## The Sampling Distribution of the OLS Estimator

-   Because $\hat{\beta_0}$ and $\hat{\beta_1}$ are computed from a sample, the estimators themselves are random variables with a probability distribution
-   This probability distribution of r.v.s is the so-called sampling distribution of the estimators. This describes the values the r.v.s could take on over different samples
-   We know that in large samples, and if the assumptions of OLS hold, the $\hat{\beta_0}$ and $\hat{\beta_1}$ have joint normal distribution.
-   Large sample normal distribution for $\hat{\beta_1}$ is $Norm(\beta_1, \sigma^2_{\hat{\beta_1}})$ where $\sigma^2_{\hat{\beta_1}} = \frac{1}{n} \frac{Var[(X_i - \mu_X)u_i]}{[Var(X_i)]^2}$
-   Large sample normal distribution for $\hat{\beta_0}$ is $Norm(\beta_0, \sigma^2_{\hat{\beta_0}})$ where $\sigma^2_{\hat{\beta_0}} = \frac{1}{n} \frac{Var(H_i u_i)}{[E(H_i^2 )]^2}$
-   the $E(\hat{\beta_0}) = \beta_0$ and $E(\hat{\beta_1}) = \beta_1$
-   The sampling distribution of the betas approaches normal distribution

```{r}
# simulate data
N <- 100000
X <- runif(N, min = 0, max = 20)
u <- rnorm(N, sd = 10)

# population regression
Y <- -2 + 3.5 * X + u
population <- data.frame(X, Y)

# The above constitutes the population which generally is unknown in the real world
```

```{r}
# let is now draw samples from the above population

# set sample size
n <- 100

# compute the variance of beta_hat_0
H_i <- 1 - mean(X) / mean(X^2) * X
var_b0 <- var(H_i * u) / (n * mean(H_i^2)^2 )

# compute the variance of hat_beta_1
var_b1 <- var( ( X - mean(X) ) * u ) / (n * var(X)^2)

# print variances to the console
var_b0

var_b1
```

-   Since the $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimates of population parameters and they can change depending on the sample drawn, let us look at multiple samples from the same population

```{r}
# set repetitions and sample size
n <- 100
reps <- 10000

# initialize the matrix of outcomes
fit <- matrix(ncol = 2, nrow = reps)

# loop sampling and estimation of the coefficients
for (i in 1:reps){
  
 sample <- population[sample(1:N, n), ]
 fit[i, ] <- lm(Y ~ X, data = sample)$coefficients
 
}

# compute variance estimates using outcomes
var(fit[, 1])

var(fit[, 2])

```

-   Let's visualize these

### Large sample size leads the estimate closer to pupulation parameter

```{r}
# divide plotting area as 1-by-2 array
par(mfrow = c(1, 2))

# plot histograms of beta_0 estimates
hist(fit[, 1],
     cex.main = 0.8,
     main = bquote(The ~ Distribution  ~ of ~ 10000 ~ beta[0] ~ Estimates), 
     xlab = bquote(hat(beta)[0]), 
     freq = F)

# add true distribution to plot
curve(dnorm(x, 
            -2, 
            sqrt(var_b0)), 
      add = T, 
      col = "darkred",lwd=2)

# plot histograms of beta_hat_1 
hist(fit[, 2],
    cex.main = 0.8,
     main = bquote(The ~ Distribution  ~ of ~ 10000 ~ beta[1] ~ Estimates), 
     xlab = bquote(hat(beta)[1]), 
     freq = F)

# add true distribution to plot
curve(dnorm(x, 
            3.5, 
            sqrt(var_b1)), 
      add = T, 
      col = "darkred",lwd=2)
```

-   The above exercise shows that for large samples, the sample estimates come closer to the population parameters considering sampling distribution.

### Consistency of estimators

-   Estimators converge in probability to the true parameters - estimates asymptotically unbiased and their variances converge to\
    $0$ as $n$ increases.
-   We can check this by repeating the simulation above for a sequence of increasing sample sizes.

```{r}
# set seed for reproducibility
set.seed(1)

# set repetitions and the vector of sample sizes
reps <- 1000
n <- c(100, 250, 1000, 3000)

# initialize the matrix of outcomes
fit <- matrix(ncol = 2, nrow = reps)

# divide the plot panel in a 2-by-2 array
par(mfrow = c(2, 2))

# loop sampling and plotting

# outer loop over n
for (j in 1:length(n)) {
  
  # inner loop: sampling and estimating of the coefficients
  for (i in 1:reps){
    
    sample <- population[sample(1:N, n[j]), ]
    fit[i, ] <- lm(Y ~ X, data = sample)$coefficients
    
  }
  
  # draw density estimates
  plot(density(fit[ ,2]), xlim=c(2.5, 4.5), 
       col = j, 
       main = paste("n=", n[j]), 
       xlab = bquote(hat(beta)[1]))
  
}
```

-   The above simulation shows that, as n increases, the distribution of $\hat{β_1}$ concentrates around its mean, i.e., its variance decreases.
-   The same behavior can be observed if we analyze the distribution of $\hat{β_0}$ instead.

### Variation in regressor reduces variance of $\hat{β_1}$

-   As we increase the amount of information (variation) provided by the regressor, that is, increasing, which is used to estimate $β_1$, we become more confident that the estimate is close to the true value.

```{r}
# load the MASS package
library(MASS)

# set seed for reproducibility
set.seed(4)

# simulate bivariate normal data
bvndata <- mvrnorm(100, 
                mu = c(5, 5), 
                Sigma = cbind(c(5, 4), c(4, 5))) 

# assign column names / convert to data.frame
colnames(bvndata) <- c("X", "Y")
bvndata <- as.data.frame(bvndata)

# subset the data
set1 <- subset(bvndata, abs(mean(X) - X) > 1)
set2 <- subset(bvndata, abs(mean(X) - X) <= 1)

# plot both data sets
plot(set1, 
     xlab = "X", 
     ylab = "Y", 
     pch = 19)

points(set2, 
       col = "steelblue", 
       pch = 19)
legend("topleft", 
       legend = c("Set1", 
                  "Set2"),
       cex = 1,
       pch = 19,
       col = c("black","steelblue"))
```

```{r}
# estimate both regression lines
lm.set1 <- lm(Y ~ X, data = set1)
lm.set2 <- lm(Y ~ X, data = set2)

# plot observations
plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)

# add both lines to the plot
abline(lm.set1, col = "black",lwd=2)
abline(lm.set2, col = "steelblue",lwd=2)
legend("bottomright", 
       legend = c("Set1", 
                  "Set2"),
       cex = 1,
       lwd=2,
       col = c("black","steelblue"))

# This shows that the dataset with high variation in regressor fits the line better than that of the set with lesser variation in regressor.
```

------------------------------------------------------------------------
